[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDLD 710: Data Analysis for Problems of Practice",
    "section": "",
    "text": "The purpose of this document is to assemble some advice and resources to support Swedish Pharmacy residents in their journey from project proposal to a completed research project for presentation and publication."
  },
  {
    "objectID": "Design.html",
    "href": "Design.html",
    "title": "Planning the Project",
    "section": "",
    "text": "The resident researcher should begin the project with a basic sense of the design logic of the project. The point of this section is to introduce or reinforce a few basic concepts of research design as they apply to the resident research project.\nThe project aspires to make a case for a proposed improved medical treatment. This case rests on causal evidence that the proposed treatment is more clinically effective than the current mode of treatment. The most convincing case would demonstrate that the treatment alone caused the improvement and cast doubt that the improvement would have happened anyway. To thoughtfully plan data collection for the strongest causal evidence, and to anticipate and minimize challenges of rival explanations, is the purpose of research design (Campbell and Stanley 1963).\nThe ideal design would be based on random assignment of patients to conditions, as in a randomly controlled trial. The resident researcher would randomly assign patients to a control condition and various treatment conditions, then compare outcomes of all these groups following treatment, and the outcomes will differ at least slightly. Assume patients in the treatment conditions had better outcomes than patients in the control condition. The resident could attribute this difference to the treatment alone because in all other ways the groups would differ only by chance by design.\nRandom assignment is probably not an option for resident research project. Until that happens, the project falls under the category of quasi-experiment which means it is more vulnerable to confounding explanations of any differences in outcomes.\nThe resident research project based on retrospective chart review will culminate in a comparison between two groups: a pre-implementation group and a post-implementation group. Assume a set of data that show better outcomes for the post-implementation group. Great! Can we attribute that to implementation of an improved treatment or protocol, or for some other reason would the post-implementation group have fared better anyway?\nOne potentially confounding pre-existing difference between the two groups is time, or history. A recent resident project provides a good example. In this project, the time frame for the pre-implementation group was early 2020 and thus this group had a higher incidence of COVID-19 infection than the more recent post-implementation group. Were outcomes for the pre-implementation group less favorable because more of them were infected with COVID-19? The proposed dosing protocol may very well have improved outcomes for the post-implementation group. The problem is there are competing explanations of the results. Pre-existing differences between the two groups in COVID-19 infection amounted to a confounding variable due to history.\nThe point here is not to teach a course in research design but to help the resident researcher clarify for this project:\n\nWhat are the primary outcomes to improve? Length of stay? Time-to-therapeutic level? These are dependent variables. They depend on, or are the effects of, other variables.\nWhat is the difference in treatment intended to cause the improvement in the post-implementation group? This is the independent variable.\nAll other variables are control variables. They should differ only by chance. If there is a noticeable pre-existing difference, and the level of that factor in one group affects the outcome, it is a confound."
  },
  {
    "objectID": "Design.html#how-many-patients",
    "href": "Design.html#how-many-patients",
    "title": "Planning the Project",
    "section": "How many patients?",
    "text": "How many patients?\nPossibly the most pressing question for resident research projects is: How many patients do I need?\nThe resident research project will culiminate in a series of comparisons between the pre-implementation and post-implementation groups. Outcomes of the two samples will differ by at least some quantity. The resident researcher expresses this difference as an effect, like this:\n[insert table about here]\nAssume that this effect suggests more favorable outcomes for the post-implementation group. This effect raises several questions:\n\nHow do we evaluate this effect?\nCould we attribute it to chance? (because it would be very unlikely for both groups to have exactly the same outcomes)\nOr is it larger than that?\n\nIn statistical terms, this is a question of statistical power. Power is the ability to isolate a treatment effect when it really does exist (Cohen 1988). Power is a function of effect size, sample size, and statistical significance. In order to decide on a number of patients we need to have a sense of what size of effect we want to reliably detect.\n\n\n\n\n\nThis a plot of power rates against these other variables. A large effect (r ~ 0.5) is detectable with a sample of any size. But only large samples have the power to detect a statistically significant (p < .05) small correlation (r = 0.1).\n\n\n\n\nCampbell, Donald T., and Julian C. Stanley. 1963. Experimental and Quasi-Experimental Designs for Research. Houghton Mifflin Company.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Mahwah, New Jersey: Lawrence Erlbaum Associates."
  },
  {
    "objectID": "Collecting.html",
    "href": "Collecting.html",
    "title": "Collecting your Data",
    "section": "",
    "text": "For collecting data from Epic, it might be helpful to have a sense of the landscape of its different databases. There are three primary databases:\nChronicles. This is the database that is collecting data from Hyperspace in real time. For reporting, Reporting Workbench pulls data directly from Chronicles, but it is otherwise not designed very well for historical reporting.\nClarity. This is the primary relational database for reporting Epic data. Through a nightly process known as ETL (“extract-transform-load”), Clarity extracts data from Chronicles and stores it in a thousand bazillion tables (think “spreadsheets”). To pull data for a report from Clarity is to identify the correct tables and fields and to write a SQL query to join the tables, apply the appropriate selection criteria, and report the appropriate fields.\nCaboodle. This a relatively new relational database that functions essentially the same as Clarity but is designed to be much easier to use. Caboodle uses fewer tables derived from myriad Clarity tables which vastly simplifies the work and complexity of writing a SQL query. The down side is not all Clarity data are in Caboodle."
  },
  {
    "objectID": "Collecting.html#granularity-of-data",
    "href": "Collecting.html#granularity-of-data",
    "title": "Collecting your Data",
    "section": "Granularity of data",
    "text": "Granularity of data\nGranularity means two related things. One is the size of the data point which is to say what context it provides for other, smaller, data points. The other is, essentially this question: What does a row in the spreadsheet mean? There are several different levels of granularity:\nPatient-level data. A patient has a unique ID number: the MRN. No two patients have the same MRN. When it comes to mining Epic data for the research project, the patient list is perhaps the most important: the resident needs a “patient list”. When it comes to data mining, the patient “level” is context to more granular data in the sense that a patient can have multiple encounters - and thus multiple Encounter CSNs “within” the same Patient MRN.\nEncounter-level data. The unique Epic ID number for the encounter is the CSN. The encounter is context to more granular data such as a treatment regimen of a particular medicine. Multiple drug administrations can occur “within” an encounter CSN.\nMedication administration level data. This is possibly the lowest level and the most granular data. In Caboodle, each administration of a medicine has a unique ID number and is time-stamped. My queries to date have been for counts of medicine administrations, or firsts, lasts, minimums and maximum doses within a hospital encounter or ICU stay.\nLab results level data. Lab data is similar to medicine administration data because, again in Caboodle, each lab result is has its own unique ID number and is time-stamped. There can be a great many lab results within a hospital encounter. My queries to date have been for counts of lab results, or firsts, lasts, minimums and maximum values within a hospital encounter or ICU stay.\nThe resident data collection form is designed for patient level data; each row in the spreadsheet captures the experience of a hospital encounter. It can also be helpful to report the medicine administration and lab result data sorted chronologically by patient and encounter."
  },
  {
    "objectID": "Collecting.html#advice-for-data-collection-and-management",
    "href": "Collecting.html#advice-for-data-collection-and-management",
    "title": "Collecting your Data",
    "section": "Advice for Data Collection and Management",
    "text": "Advice for Data Collection and Management\nBe proactive. As you begin to decide what data to collect, submit your project plan and requests for data in writing to me (Jack) as soon as possible. This is so I can have a bit of time to understand your project, do some discovery in Caboodle or Clarity, and note any questions. Then meet with me via Teams to get on the same page.\nDevise a system for organizing your data. By this I mean version control. This data collection process is iterative. You will ask for data and I’ll write a SQL query that produces an Excel workbook of data. That’s one iteration. In all likelihood you’ll need a revision or two. Upon receiving your feedback I’ll edit my query and produce for you a new set of data to replace the first set. That’s the second iteration. The more iterations, the more data, the more potential for multiple versions and data overload. And I may not be able to do more than a few iterations. I can work on some best practices to help us through this. Stay tuned.\nThere is no substitute for chart review. Some fraction of resident research data can come from Epic data mining, but it still needs to be validated with careful chart review. And some data which are very difficult to mine or to query into the correct format may have to come from chart review."
  },
  {
    "objectID": "Preparing.html",
    "href": "Preparing.html",
    "title": "1  Preparing your Data",
    "section": "",
    "text": "2 Best practices for Excel\nIn all likelihood the primary tool you’ll use for working with your data is Excel - possibly by now the most commonly used tool for working with data in the world. What follows is a list of good practices that might make your Excel data easier to manage and collaborate with others. These tips will save you time wasted on fixing data and will help keep your data in format appropriate for analysis:\n\nPut variables in columns and observations in rows. Include a unique identifying number for each case. Be sure that each variable name is unique (no duplicate variable names).\nPut variable names in the first row. Variables must start with a letter. Do not include special characters (#, !, ?, %, etc.) or spaces in your variable names.\nKeep all your data “touching”. No empty rows or columns. This is critically important for sorting. Empty columns or rows break the structural integrity of your data set and could allow you to sort a subsection of your data apart the rest of it.\nNo merged cells.\nUse a separate column for each piece of information. Don’t enter data such as “120/80” for blood pressure. Enter systolic blood pressure as one variable and diastolic blood pressure as another variable. Don’t enter data as “A,C,D” or “BDF” if there are three possible answers to a question. Include a separate column for each answer.\nDecide on a “missingness” convention. Missing data can cause a multitude of problems. To enter a missing data value either enter a blank or an “impossible” numeric code (for numbers) or an easily recognizable single digit character code for character (trying to avoid mixing numeric and character data). Be sure, if you use a missing value code, that it cannot be confused with a “real” data value.\nUse only one worksheet for your data; do analysis on a different worksheet. If you decide to use multiple sheets for you data, follow the variable naming conventions for the tabs that name the sheets (keep the names simple and unique).\nDo not “stack” data on the same sheets. For example, “treated” versus “non-treated” patients can be handled by column variable that has a code for Treated (yes/no).\nDedicate one worksheet to your original, unedited raw data. Make a copy of it to do all your cleaning and analysis. You might label this worksheet “Original” or “Raw data.” This is important so that when you make a mistake, you always have your original data to fall back on.\nDedicate one worksheet to your Clean / Working data.\nMake the most of your variable labels. On your worksheet of “Clean” (or “Working”) data, make sure every column of data has a clear, concise, descriptive label. Here’s what I do to take column labels to the next level:\n\n\nEnsure that the top row of my data includes a clear, concise label for each column of data.\nBold the row.\nAdd a fill color to the row.\nFreeze the row (Select the row, then View –> Freeze top row).\nEnable word wrap in the row.\n\n\nApply a consistent format for your columns. Data elements are different sizes. Names tend to be long while numerical values tend to be short. I don’t like it when a column label is left-aligned but the data are right-aligned. I find these variations in visual formatting distracting. To deal with these distractions, I tend to:\n\n\nApply all the column label formatting mentioned above.\nFix all my column widths to 15.\nLeft align columns (both column labels and data) for text (patient names, medication names, etc.).\nCenter columns (both column labels and data) for numeric values.\nRight align columns for time data.\n\nI find (and I think you will too) that enforcing a consistent format removes variable formatting as a distraction so I can see and focus on the data.\n\n\n\n3 Excel skills\nHere are the skills you will most likely need and use:\n\nSorting your data array on a column\nFiltering your data array based on specific values of one or more columns\nFill down\nPivot Table\nVLOOKUP function - to matching together related data from different sources\nConditional formatting\nBasic calculations (SUM, AVG, COUNTIF, etc.)\nCONCATENATE function - to stitch together text and values from different data columns into a new column (which is sometimes helpful and necessary but is generally bad data practice to be avoided)\n\n\n\n\n4 Resources for data preparation\nTidy Data, by Hadley Wickham (a well-known data scientist), is a classic paper that defines what makes data clean (or “tidy”) [@WickhamTidy]\nThe University of New Hampshire Library has an excellent research guide for using Excel, including data cleaning, data analysis, data visualization, and spreadsheet best practices.\nPreparing Data in Excel, from the University of Nebraska Medical Center College of Public Health, has an excellent set of guidelines for working with Excel\nIntroduction to Excel is an excellent online module from the University of South Australia Research Methodologies and Statistics department.\nAnalysis Ready Datasets is an excellent resource from Harvard Medical School\n\n\n\n5 Granularity of data\nGranularity of data means two related things worth your attention:\n\nOne is the size of the data point, which is to say what context it provides for other, smaller, data points. Put another way, what data points do you intend to count or summarize, and by which groups do you intend to compare these summaries?\nThe other is, essentially this question: What does a row in the spreadsheet mean? Because Excel counts rows, but what’s contained in a row may not be what you intend to count.\n\nHere are several different levels of granularity of Epic data:\nPatient level. A patient has a unique ID number: the MRN. No two patients have the same MRN. When it comes to mining Epic data for the research project, the patient list is perhaps the most important: the resident needs a “patient list”. When it comes to data mining, the patient “level” is context to more granular data in the sense that a patient can have multiple encounters - and thus multiple Encounter CSNs “within” the same Patient MRN.\nEncounter level. The unique Epic ID number for the encounter is the CSN. The encounter is context to more granular data such as a treatment regimen of a particular medicine. Multiple drug administrations can occur “within” an encounter CSN.\nMedication administration level. This is possibly the lowest level and the most granular data. In Caboodle, each administration of a medicine has a unique ID number and is time-stamped. My queries to date have been for counts of medicine administrations, or firsts, lasts, minimums and maximum doses within a hospital encounter or ICU stay.\nLab results level. Lab data is similar to medicine administration data because, again in Caboodle, each lab result is has its own unique ID number and is time-stamped. There can be a great many lab results within a hospital encounter. My queries to date have been for counts of lab results, or firsts, lasts, minimums and maximum values within a hospital encounter or ICU stay.\nThe resident data collection form is designed for patient level data; each row in the spreadsheet captures the experience of a hospital encounter. It can also be helpful to report the medicine administration and lab result data sorted chronologically by patient and encounter.\n\n# References {-}\n\n\nCampbell, Donald T., and Julian C. Stanley. 1963. Experimental and\nQuasi-Experimental Designs for Research. Houghton Mifflin Company.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Mahwah, New Jersey: Lawrence Erlbaum Associates.\n\n\nDe Muth, James E. 2009. “Overview of Biostatistics Used in\nClinical Research.” American Journal of Health-System\nPharmacy 66: 70–81."
  },
  {
    "objectID": "Analyzing.html",
    "href": "Analyzing.html",
    "title": "2  Analyzing your Data",
    "section": "",
    "text": "The purpose of this page is to give you tools to analyze your data using appropriate statistics. There are two important tasks:"
  },
  {
    "objectID": "Analyzing.html#nominal-measurement",
    "href": "Analyzing.html#nominal-measurement",
    "title": "2  Analyzing your Data",
    "section": "3.1 Nominal measurement",
    "text": "3.1 Nominal measurement\nNominal measurement is categories. Each patient must fall into only one category, and the categories must be mutually exclusive and exhaustive. Here are some examples:\n\nGender (Male/Female)\nRacial identity\nMarital status\nControl group/Experimental group\nInfected with COVID vs. not infected with COVID\nDisease presence\nMortality\n\nOutcomes are usually reported as frequency counts or percentages (in each category)."
  },
  {
    "objectID": "Analyzing.html#ordinal-measurement",
    "href": "Analyzing.html#ordinal-measurement",
    "title": "2  Analyzing your Data",
    "section": "3.2 Ordinal measurement",
    "text": "3.2 Ordinal measurement\nOrdinal measure also puts patients into categories, but the categories have an ascending or descending order: patients have more or less of somthing. But the differences between the categories is not necessarily the same. Here are some examples:\n\nStages I-IV tumors\n0-10 Apgar scores\n\nA Stage IV tumor is more advanced than a Stage II tumor, but not necessarily by twice as much. A Stage III tumor is more advanced than a Stage I tumor, but not necessarily by three times as much.\nFor this reason, we cannot perform arithmetic or calculate means or other parametric statistics on ordinal values.\nHowever, if your project has an ordinal level outcome on which you need to compare treatment groups, there are appropriate nonparametric statistics you can use to see which group is significantly more of this outcome than another. Examples include:\n\nchi-square \\(\\chi 2\\) statistics\nthe Mann-Whitney U test\nthe Spearman’s rho test"
  },
  {
    "objectID": "Analyzing.html#interval-and-ratio-level-measurement",
    "href": "Analyzing.html#interval-and-ratio-level-measurement",
    "title": "2  Analyzing your Data",
    "section": "3.3 Interval and ratio level measurement",
    "text": "3.3 Interval and ratio level measurement\nFinally, interval and ratio measurement means continuous data: patients fall somewhere on a continuum, like a temperature scale. As a result, variables measured using interval and ratio scales are often referred to as continuous variables. Here are some examples:\n\nHeight\nWeight\nCholesterol level\nBlood pressure\nTime\n\nOn variables like these there is relative positioning with no gaps or interruptions in the continuum.\nThe difference between interval and ratio scales is that ratio has a true zero value while interval does not.\nOn variables like these it is permissible to do arithmetic and to summarize them with the mean and standard deviation which, in turn, avail to you more commonly used advanced statistics like:\n\nt tests\nanalyses of variance (ANOVA)\ncorrelation\nregression\n\nOnce you have a good feel for the measurement levels of your outcome and predictor variables, you can choose appropriate statistics. (Simpson2015?) offers two decision trees to help you make these choices:"
  },
  {
    "objectID": "Analyzing.html#external-resources-for-data-analysis",
    "href": "Analyzing.html#external-resources-for-data-analysis",
    "title": "2  Analyzing your Data",
    "section": "4.1 External resources for data analysis",
    "text": "4.1 External resources for data analysis\nHere are a few links to external resources on data analysis and statistics.\n\nThe R Psychologist, by @magnussonCohend - is an outstanding resource to better understand statistics\nOnline Modules in Research Methods and Data Analysis at the University of South Australia\nData Analysis from the University of New Hampshire"
  },
  {
    "objectID": "Analyzing.html#excel-based-tools",
    "href": "Analyzing.html#excel-based-tools",
    "title": "2  Analyzing your Data",
    "section": "5.1 Excel-based tools",
    "text": "5.1 Excel-based tools\n\nEZAnalyze is a simple Excel add-in for data analysis. It includes menus for selecting various statistics from your variables.\nXLStat\nData Analysis native add-in"
  },
  {
    "objectID": "Analyzing.html#data-analysis-software",
    "href": "Analyzing.html#data-analysis-software",
    "title": "2  Analyzing your Data",
    "section": "5.2 Data analysis software",
    "text": "5.2 Data analysis software\n\nR @R-base, with RStudio and R Markdown, is free open source software for data analysis, statistics, and data visualization. It is powerful and flexible but it does require ongoing learning of code because it is constantly evolving.\n\nHere are several other robust software applications for data analysis available to you. One or more of them may be free for you as a Gonzaga student:\n\nJMP - JMP is a suite of software used for statistical analysis\nSAS - The SAS System is a comprehensive statistical software package from SAS Institute for data management, graphics, analysis, and presentation\nSPSS - IBM SPSS (Statistical Package for the Social Sciences) provides data and statistical analysis, file management capabilities, graphics and reporting features.\n\n\n\n\nDe Muth, James E. 2009. “Overview of Biostatistics Used in Clinical Research.” American Journal of Health-System Pharmacy 66: 70–81."
  },
  {
    "objectID": "Reporting.html",
    "href": "Reporting.html",
    "title": "3  Reporting your Data",
    "section": "",
    "text": "The purpose of this final page is to help you decide how best to package and present the results of your data analysis for a professional audience."
  },
  {
    "objectID": "Reporting.html#line-graphs-for-over-time-data",
    "href": "Reporting.html#line-graphs-for-over-time-data",
    "title": "3  Reporting your Data",
    "section": "4.1 1. Line graphs for over-time data",
    "text": "4.1 1. Line graphs for over-time data\nLine graphs are the appropriate way to show change over time in one or a few groups. Your x-axis (horizontal) should be time, and your y-axis should be the quantity by which you want to see change over time. You can use different lines for different groups."
  },
  {
    "objectID": "Reporting.html#bar-graphs-for-group-comparisons.",
    "href": "Reporting.html#bar-graphs-for-group-comparisons.",
    "title": "3  Reporting your Data",
    "section": "4.2 2. Bar graphs for group comparisons.",
    "text": "4.2 2. Bar graphs for group comparisons.\nThe bar graph is the Swiss army knife of data visualization. It’s useful because it is so versatile. Bar graphs use size to compare different quantities. Your y-axis is your quantity and on your x-axis you put your group categories."
  },
  {
    "objectID": "Reporting.html#save-the-pies-for-dessert",
    "href": "Reporting.html#save-the-pies-for-dessert",
    "title": "3  Reporting your Data",
    "section": "4.3 3. Save the pies for dessert",
    "text": "4.3 3. Save the pies for dessert\nPie graphs are a great way to visualize proportions - parts that make up a whole. And, with a few nice colors, they’re attractive. They’re also simple.\nBut they can be a pain to create - to get right visually. They also lose their utility when you have more than a few categories. For this reason, the AMA discourages the use of pie graphs:\nIf you do want to use a pie graph, my advice to you is to keep it simple; use it only to show a few categories."
  }
]