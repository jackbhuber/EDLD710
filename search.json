[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDLD 710: Data Analysis for Problems of Practice",
    "section": "",
    "text": "This site was built in R (R Core Team 2022) with Quarto (Allaire 2022).↩︎"
  },
  {
    "objectID": "Preparing.html",
    "href": "Preparing.html",
    "title": "Data Preparation",
    "section": "",
    "text": "The purpose of this page is to help you clean, organize, and otherwise prepare and enhance your data for statistical analysis and/or visualization."
  },
  {
    "objectID": "Preparing.html#best-practices-for-excel",
    "href": "Preparing.html#best-practices-for-excel",
    "title": "Data Preparation",
    "section": "Best practices for Excel",
    "text": "Best practices for Excel\nIn all likelihood the primary tool you’ll use for working with your data is Excel - possibly by now the most commonly used tool for working with data in the world. What follows is a list of good practices that might make your Excel data easier to manage and collaborate with others. These tips will save you time wasted on fixing data and will help keep your data in format appropriate for analysis:\nPut variables in columns and observations in rows. Include a unique identifying number for each case. Be sure that each variable name is unique (no duplicate variable names).\nPut variable names in the first row. Variables must start with a letter. Do not include special characters (#, !, ?, %, etc.) or spaces in your variable names.\nKeep all your data “touching”. No empty rows or columns. This is critically important for sorting. Empty columns or rows break the structural integrity of your data set and could allow you to sort a subsection of your data apart the rest of it.\nNo merged cells.\nUse a separate column for each piece of information. Don’t enter data such as “120/80” for blood pressure. Enter systolic blood pressure as one variable and diastolic blood pressure as another variable. Don’t enter data as “A,C,D” or “BDF” if there are three possible answers to a question. Include a separate column for each answer.\nDecide on a “missingness” convention. Missing data can cause a multitude of problems. To enter a missing data value either enter a blank or an “impossible” numeric code (for numbers) or an easily recognizable single digit character code for character (trying to avoid mixing numeric and character data). Be sure, if you use a missing value code, that it cannot be confused with a “real” data value.\nUse only one worksheet for your data; do analysis on a different worksheet. If you decide to use multiple sheets for you data, follow the variable naming conventions for the tabs that name the sheets (keep the names simple and unique).\nDo not “stack” data on the same sheets. For example, “treated” versus “non-treated” patients can be handled by column variable that has a code for Treated (yes/no).\nDedicate one worksheet to your original, unedited raw data. Make a copy of it to do all your cleaning and analysis. You might label this worksheet “Original” or “Raw data.” This is important so that when you make a mistake, you always have your original data to fall back on.\nDedicate one worksheet to your Clean / Working data.\nMake the most of your variable labels. On your worksheet of “Clean” (or “Working”) data, make sure every column of data has a clear, concise, descriptive label. Here’s what I do to take column labels to the next level:\n\nEnsure that the top row of my data includes a clear, concise label for each column of data.\nBold the row.\nAdd a fill color to the row.\nFreeze the row (Select the row, then View –> Freeze top row).\nEnable word wrap in the row.\n\nApply a consistent format for your columns. Data elements are different sizes. Names tend to be long while numerical values tend to be short. I don’t like it when a column label is left-aligned but the data are right-aligned. I find these variations in visual formatting distracting. To deal with these distractions, I tend to:\n\nApply all the column label formatting mentioned above.\nFix all my column widths to 15.\nLeft align columns (both column labels and data) for text (patient names, medication names, etc.).\nCenter columns (both column labels and data) for numeric values.\nRight align columns for time data.\n\nI find (and I think you will too) that enforcing a consistent format removes variable formatting as a distraction so I can see and focus on the data."
  },
  {
    "objectID": "Preparing.html#excel-skills",
    "href": "Preparing.html#excel-skills",
    "title": "Data Preparation",
    "section": "Excel skills",
    "text": "Excel skills\nHere are the skills you will most likely need and use:\n\nSorting your data array on a column\nFiltering your data array based on specific values of one or more columns\nFill down\nPivot Table\nVLOOKUP function - to matching together related data from different sources\nConditional formatting\nBasic calculations (SUM, AVG, COUNTIF, etc.)\nCONCATENATE function - to stitch together text and values from different data columns into a new column (which is sometimes helpful and necessary but is generally bad data practice to be avoided)"
  },
  {
    "objectID": "Preparing.html#resources-for-data-preparation",
    "href": "Preparing.html#resources-for-data-preparation",
    "title": "Data Preparation",
    "section": "Resources for data preparation",
    "text": "Resources for data preparation\nTidy Data, by Hadley Wickham (a well-known data scientist), is a classic paper that defines what makes data clean (or “tidy”) [@WickhamTidy]\nThe University of New Hampshire Library has an excellent research guide for using Excel, including data cleaning, data analysis, data visualization, and spreadsheet best practices.\nPreparing Data in Excel, from the University of Nebraska Medical Center College of Public Health, has an excellent set of guidelines for working with Excel\nIntroduction to Excel is an excellent online module from the University of South Australia Research Methodologies and Statistics department.\nAnalysis Ready Datasets is an excellent resource from Harvard Medical School"
  },
  {
    "objectID": "Preparing.html#granularity-of-data",
    "href": "Preparing.html#granularity-of-data",
    "title": "Data Preparation",
    "section": "Granularity of data",
    "text": "Granularity of data\nGranularity of data means two related things worth your attention:\n\nOne is the size of the data point, which is to say what context it provides for other, smaller, data points. Put another way, what data points do you intend to count or summarize, and by which groups do you intend to compare these summaries?\nThe other is, essentially this question: What does a row in the spreadsheet mean? Because Excel counts rows, but what’s contained in a row may not be what you intend to count.\n\nHere are several different levels of granularity of Epic data:\nPatient level. A patient has a unique ID number: the MRN. No two patients have the same MRN. When it comes to mining Epic data for the research project, the patient list is perhaps the most important: the resident needs a “patient list”. When it comes to data mining, the patient “level” is context to more granular data in the sense that a patient can have multiple encounters - and thus multiple Encounter CSNs “within” the same Patient MRN.\nEncounter level. The unique Epic ID number for the encounter is the CSN. The encounter is context to more granular data such as a treatment regimen of a particular medicine. Multiple drug administrations can occur “within” an encounter CSN.\nMedication administration level. This is possibly the lowest level and the most granular data. In Caboodle, each administration of a medicine has a unique ID number and is time-stamped. My queries to date have been for counts of medicine administrations, or firsts, lasts, minimums and maximum doses within a hospital encounter or ICU stay.\nLab results level. Lab data is similar to medicine administration data because, again in Caboodle, each lab result is has its own unique ID number and is time-stamped. There can be a great many lab results within a hospital encounter. My queries to date have been for counts of lab results, or firsts, lasts, minimums and maximum values within a hospital encounter or ICU stay.\nThe resident data collection form is designed for patient level data; each row in the spreadsheet captures the experience of a hospital encounter. It can also be helpful to report the medicine administration and lab result data sorted chronologically by patient and encounter."
  },
  {
    "objectID": "Analyzing.html",
    "href": "Analyzing.html",
    "title": "Data Analysis",
    "section": "",
    "text": "The purpose of this page is to give you tools to analyze your data using appropriate statistics. There are two important tasks:"
  },
  {
    "objectID": "Analyzing.html#define-the-measurement-levels-of-your-variables",
    "href": "Analyzing.html#define-the-measurement-levels-of-your-variables",
    "title": "Data Analysis",
    "section": "Define the measurement levels of your variables",
    "text": "Define the measurement levels of your variables\nIt is important to know the measurement level of your variables (De Muth 2009). How do you express the outcome by which to compare your pre- and post- samples? Is it…\n\nPercent of patients who achieve an initial therapeutic goal?\nTime to initial therapeutic level?\nPercent of patients who experience an adverse outcome (such as acute kidney injury)?\nMortality rate (% surviving)? In such a case you would be comparing two proportions.\n“Time to…” a therapeutic level? In such a case you would be comparing two different quantities of time.\n\n\nNominal measurement\nNominal measurement is categories. Each patient must fall into only one category, and the categories must be mutually exclusive and exhaustive. Here are some examples:\n\nGender (Male/Female)\nRacial identity\nMarital status\nControl group/Experimental group\nInfected with COVID vs. not infected with COVID\nDisease presence\nMortality\n\nOutcomes are usually reported as frequency counts or percentages (in each category).\n\n\nOrdinal measurement\nOrdinal measure also puts patients into categories, but the categories have an ascending or descending order: patients have more or less of somthing. But the differences between the categories is not necessarily the same. Here are some examples:\n\nStages I-IV tumors\n0-10 Apgar scores\n\nA Stage IV tumor is more advanced than a Stage II tumor, but not necessarily by twice as much. A Stage III tumor is more advanced than a Stage I tumor, but not necessarily by three times as much.\nFor this reason, we cannot perform arithmetic or calculate means or other parametric statistics on ordinal values.\nHowever, if your project has an ordinal level outcome on which you need to compare treatment groups, there are appropriate nonparametric statistics you can use to see which group is significantly more of this outcome than another. Examples include:\n\nchi-square \\(\\chi 2\\) statistics\nthe Mann-Whitney U test\nthe Spearman’s rho test\n\n\n\nInterval and ratio level measurement\nFinally, interval and ratio measurement means continuous data: patients fall somewhere on a continuum, like a temperature scale. As a result, variables measured using interval and ratio scales are often referred to as continuous variables. Here are some examples:\n\nHeight\nWeight\nCholesterol level\nBlood pressure\nTime\n\nOn variables like these there is relative positioning with no gaps or interruptions in the continuum.\nThe difference between interval and ratio scales is that ratio has a true zero value while interval does not.\nOn variables like these it is permissible to do arithmetic and to summarize them with the mean and standard deviation which, in turn, avail to you more commonly used advanced statistics like:\n\nt tests\nanalyses of variance (ANOVA)\ncorrelation\nregression\n\nOnce you have a good feel for the measurement levels of your outcome and predictor variables, you can choose appropriate statistics. Simpson (2015) offers two decision trees to help you make these choices:"
  },
  {
    "objectID": "Analyzing.html#choose-appropriate-statistics",
    "href": "Analyzing.html#choose-appropriate-statistics",
    "title": "Data Analysis",
    "section": "Choose appropriate statistics",
    "text": "Choose appropriate statistics\nThis section offers more information on several choice statistics. These are statistics I’ve used for recent resident projects and seen in the journals.\n\nChi-square \\(\\chi 2\\) tests\nThe chi-square \\(\\chi 2\\) test is a commonly used statistic for nominal/categorical data. We use it to examine the distribution of cases across categories. Essentially, it compares the distribution of cases you actually see to the distribution of cases you would expect to see from normal variation.\nHere is one example of a chi-square \\(\\chi 2\\) test for a recent resident project. The question is whether gender (male/female) makes a statistically significant difference in whether patients need three or more dose changes of bivalirudin before they reach a therapeutic goal.\n\n#d <- read.csv(\"data/bivalirudin.csv\") # load data\n#table_dosechgs_gender <- xtabs(~d$d_Male + d$DV_3DoseChanges, data=d) # crosstabulate \n#knitr::kable(table_dosechgs_gender, align = \"l\")\n#summary(table_dosechgs_gender) # calculate chi-square\n\nThe chi-square \\(\\chi 2\\) value of 1.9421 with one degree of freedom has a p-value of 0.1634. It is not statistically significant, suggesting that gender makes no significant difference in reaching therapeutic goal.\n\n\nt tests\nThe t test is a commonly used statistic for comparing two groups on a continuous outcome."
  },
  {
    "objectID": "Analyzing.html#external-resources-for-data-analysis",
    "href": "Analyzing.html#external-resources-for-data-analysis",
    "title": "Data Analysis",
    "section": "External resources for data analysis",
    "text": "External resources for data analysis\nHere are a few links to external resources on data analysis and statistics.\n\nThe R Psychologist, by @magnussonCohend - is an outstanding resource to better understand statistics\nOnline Modules in Research Methods and Data Analysis at the University of South Australia\nData Analysis from the University of New Hampshire"
  },
  {
    "objectID": "Analyzing.html#software-tools-for-data-analysis",
    "href": "Analyzing.html#software-tools-for-data-analysis",
    "title": "Data Analysis",
    "section": "Software tools for data analysis",
    "text": "Software tools for data analysis\nOf equal importance to the didactics of statistics are the brass tacks of software for working with statistics. Here are several software tools for analyzing data:\n\nExcel-based tools\n\nEZAnalyze is a simple Excel add-in for data analysis. It includes menus for selecting various statistics from your variables.\nXLStat\nData Analysis native add-in\n\n\n\nData analysis software\n\nR @R-base, with RStudio and R Markdown, is free open source software for data analysis, statistics, and data visualization. It is powerful and flexible but it does require ongoing learning of code because it is constantly evolving.\n\nHere are several other robust software applications for data analysis available to you. One or more of them may be free for you as a Gonzaga student:\n\nJMP - JMP is a suite of software used for statistical analysis\nSAS - The SAS System is a comprehensive statistical software package from SAS Institute for data management, graphics, analysis, and presentation\nSPSS - IBM SPSS (Statistical Package for the Social Sciences) provides data and statistical analysis, file management capabilities, graphics and reporting features\n\n\n\n\n\nDe Muth, James E. 2009. “Overview of Biostatistics Used in Clinical Research.” American Journal of Health-System Pharmacy 66: 70–81.\n\n\nSimpson, Scot. 2015. “Creating a Data Analysis Plan: What to Consider When Choosing Statistics for a Study.” Canadian Journal of Hospital Pharmacy 68 (4): 311–17."
  },
  {
    "objectID": "Reporting.html",
    "href": "Reporting.html",
    "title": "Data Presentation",
    "section": "",
    "text": "The purpose of this final page is to help you decide how best to package and present the results of your data analysis for a professional audience."
  },
  {
    "objectID": "Reporting.html#best-practices-for-figures-i.e.-graphs",
    "href": "Reporting.html#best-practices-for-figures-i.e.-graphs",
    "title": "Data Presentation",
    "section": "Best practices for figures (i.e. graphs)",
    "text": "Best practices for figures (i.e. graphs)\n“Figures should be accurate, clear, and concise. As with tables, the figure with its title and legend should be understandable without undue reference to the text.”1\n\n1. Line graphs for over-time data\nLine graphs are the appropriate way to show change over time in one or a few groups. Your x-axis (horizontal) should be time, and your y-axis should be the quantity by which you want to see change over time. You can use different lines for different groups.\n\n\n2. Bar graphs for group comparisons.\nThe bar graph is the Swiss army knife of data visualization. It’s useful because it is so versatile. Bar graphs use size to compare different quantities. Your y-axis is your quantity and on your x-axis you put your group categories.\n\n\n3. Save the pies for dessert\nPie graphs are a great way to visualize proportions - parts that make up a whole. And, with a few nice colors, they’re attractive. They’re also simple.\nBut they can be a pain to create - to get right visually. They also lose their utility when you have more than a few categories. For this reason, the AMA discourages the use of pie graphs:\nIf you do want to use a pie graph, my advice to you is to keep it simple; use it only to show a few categories."
  },
  {
    "objectID": "Reporting.html#best-practices-for-tables",
    "href": "Reporting.html#best-practices-for-tables",
    "title": "Data Presentation",
    "section": "Best practices for tables",
    "text": "Best practices for tables\nAlthough data visualization has become all the rage, there is a still a place at the table for tables (sorry - bad pun - I couldn’t resist). Tables remain a concise way to present a sizable amount of quantitative data.\nI encourage you to strive for your tables to meet this standard: “A properly designed and constructed table should be able to stand independently, without requiring undue reference to the text.”2\nI encourage you to review the section on tables in the APA Style Guide.\n\nWhat goes on the far left column?\nHow do I format the main column headings?\nHow do I report p-values?\nHow do I report my data?\nHow do I align things?\n\nThis article from Miller et al. (2020) illustrates how to present a table that reports the results of a series of chi-square tests of the differences between two groups (pre-implementation and post-implementation) on categorical outcomes.\nNotice that the authors showed the independent variable (intervention group) as columns which enables us as readers to compare outcomes by reading left to right.\n\n\n\n\nMiller, Daryl, Melissa Ramsey, Timothy R. L’Hommedieu, and Lauren Verbosky. 2020. “Pharmacist-Led Transitions-of-Care Program Reduces 30-Day Readmission Rates for Medicare Patients in a Large Health System.” American Journal of Health-System Pharmacy 77: 972–78."
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Allaire, JJ. 2022. R Interface to ’Quarto’ Markdown Publishing\nSystem. https://cran.r-project.org/web/packages/quarto/quarto.pdf.\n\n\nDe Muth, James E. 2009. “Overview of Biostatistics Used in\nClinical Research.” American Journal of Health-System\nPharmacy 66: 70–81.\n\n\nMiller, Daryl, Melissa Ramsey, Timothy R. L’Hommedieu, and Lauren\nVerbosky. 2020. “Pharmacist-Led Transitions-of-Care Program\nReduces 30-Day Readmission Rates for Medicare Patients in a Large Health\nSystem.” American Journal of Health-System Pharmacy 77:\n972–78.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSimpson, Scot. 2015. “Creating a Data Analysis Plan: What to\nConsider When Choosing Statistics for a Study.” Canadian\nJournal of Hospital Pharmacy 68 (4): 311–17."
  }
]